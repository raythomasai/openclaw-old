# OpenClaw Research v2 - Implementation Plan
## Tonight (Feb 3, 2026)

---

## ğŸ¯ Goal
Implement the core research pipeline: Scout â†’ Harvester â†’ Deduplication â†’ Output

---

## ğŸ“‹ Implementation Steps

### Phase 1: Foundation Setup (18:00-19:00)

#### 1.1 Create Project Structure
```bash
mkdir -p projects/openclaw-research-v2/{src/{agents,collectors,dedup,output,utils},data/{raw,processed},config}
```

#### 1.2 Create Config File
- `config.yaml` with API keys and source configs
- Environment variables for secrets

#### 1.3 Set Up Dependencies
```bash
cd projects/openclaw-research-v2
python3 -m venv venv
source venv/bin/activate
pip install requests tweepy praw python-dotenv redis
```

---

### Phase 2: Collector Implementation (19:00-21:00)

#### 2.1 Twitter/X Collector
- Use Twitter API v2 (or tweepy)
- Search: "OpenClaw", "@openclaw_ai", "#openclaw"
- Rate limiting: 450 req/15min
- Output: JSON lines to `data/raw/twitter.jsonl`

#### 2.2 YouTube Collector
- YouTube Data API
- Search: OpenClaw tutorials, demos, showcases
- Extract: title, description, view count, comments
- Output: `data/raw/youtube.jsonl`

#### 2.3 Reddit Collector
- Use Pushshift API (faster, unlimited)
- Search: r/OpenClaw, r/ArtificialIntelligence, r/automation
- Fallback: Reddit API if Pushshift fails
- Output: `data/raw/reddit.jsonl`

#### 2.4 GitHub Collector
- GitHub API search
- Search: OpenClaw repos, forks, stars
- Extract: README, issues, discussions
- Output: `data/raw/github.jsonl`

---

### Phase 3: Deduplication Engine (21:00-22:00)

#### 3.1 Content Fingerprinting
- SHA-256 hash of content (normalized: lowercase, stripped)
- Bloom filter for memory-efficient dedup
- Redis set for exact-match dedup

#### 3.2 Similarity Detection
- SimHash or MinHash for near-duplicate detection
- Threshold: 0.85 similarity
- Output: `data/processed/deduplicated.jsonl`

---

### Phase 4: Output Generator (22:00-23:00)

#### 4.1 Slack Formatter
- One-line summaries with links
- Batch by source
- Post to #research

#### 4.2 Document Generator
- Markdown â†’ Google Doc (via Google Docs API)
- PDF export (markdown â†’ PDF)

---

## ğŸš€ Tonight's Run Command

```bash
cd projects/openclaw-research-v2
source venv/bin/activate
python src/main.py --sources twitter youtube reddit github --output slack
```

---

## ğŸ“ File Structure

```
projects/openclaw-research-v2/
â”œâ”€â”€ config/
â”‚   â””â”€â”€ config.yaml          # API keys, settings
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ agents/
â”‚   â”‚   â”œâ”€â”€ scout.py         # Discover new sources
â”‚   â”‚   â”œâ”€â”€ harvester.py    # Pull from all sources
â”‚   â”‚   â”œâ”€â”€ analyzer.py     # Quality scoring
â”‚   â”‚   â””â”€â”€ synthesizer.py  # Generate outputs
â”‚   â”œâ”€â”€ collectors/
â”‚   â”‚   â”œâ”€â”€ twitter.py
â”‚   â”‚   â”œâ”€â”€ youtube.py
â”‚   â”‚   â”œâ”€â”€ reddit.py
â”‚   â”‚   â”œâ”€â”€ github.py
â”‚   â”‚   â””â”€â”€ hackernews.py
â”‚   â”œâ”€â”€ dedup/
â”‚   â”‚   â”œâ”€â”€ fingerprint.py
â”‚   â”‚   â””â”€â”€ dedup.py
â”‚   â”œâ”€â”€ output/
â”‚   â”‚   â”œâ”€â”€ slack.py
â”‚   â”‚   â”œâ”€â”€ docs.py
â”‚   â”‚   â””â”€â”€ pdf.py
â”‚   â””â”€â”€ utils/
â”‚       â”œâ”€â”€ rate_limiter.py
â”‚       â””â”€â”€ logger.py
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/                 # Unprocessed data
â”‚   â””â”€â”€ processed/           # Deduplicated, scored
â”œâ”€â”€ main.py                  # Entry point
â””â”€â”€ requirements.txt
```

---

## âœ… Success Criteria (End of Night)

- [ ] Collect data from 4+ sources
- [ ] Zero duplicates in final output
- [ ] Post to Slack #research with findings
- [ ] Auto-generate Markdown document
- [ ] Ready for PDF export

---

## ğŸ”‘ API Keys Needed

| Source | Status | Location |
|--------|--------|----------|
| Twitter API | âš ï¸ Check 1Password | op get item "Twitter API" |
| YouTube API | âš ï¸ Check 1Password | op get item "Google Cloud" |
| GitHub Token | âš ï¸ Check 1Password | op get item "GitHub" |
| Reddit API | âœ… Pushshift (free) | - |
| Discord Token | âš ï¸ If needed | - |

---

## ğŸ“ Post-Run Tasks

1. Review findings quality
2. Adjust scoring algorithm
3. Add missing sources
4. Schedule cron job for nightly runs

---

*Plan created: 2026-02-03 10:46 CST*
